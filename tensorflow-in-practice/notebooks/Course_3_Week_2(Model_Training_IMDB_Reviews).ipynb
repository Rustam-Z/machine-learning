{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('tf': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "4ea0e157563bacde0b7fd8dc93db6051c9678d5eadbd4117abf1a4cecbc8cd1a"
   }
  },
  "interpreter": {
   "hash": "4ea0e157563bacde0b7fd8dc93db6051c9678d5eadbd4117abf1a4cecbc8cd1a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "tf.executing_eagerly() # if 1.x use `tf.enable_eager_execution()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True) # loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "tfds.list_builders()[:5] # the list of all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test'] # 25k train and 25k testing\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "for sample, label in train_data:\n",
    "    training_sentences.append(sample.numpy().decode('utf8'))\n",
    "    training_labels.append(label.numpy())\n",
    "\n",
    "for sample, label in test_data:\n",
    "    testing_sentences.append(sample.numpy().decode('utf8'))\n",
    "    testing_labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n>> label 0\n"
     ]
    }
   ],
   "source": [
    "print(training_sentences[1]) \n",
    "print(\">> label\", training_labels[1]) # 0 negative, 1 pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25000\n25000\n25000\n25000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_sentences))\n",
    "print(len(training_labels))\n",
    "print(len(testing_sentences))\n",
    "print(len(testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to numpy arrays\n",
    "training_labels_final = np.array(training_labels) \n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "training_labels_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(25000, 120)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Preparing data for training by tokenizing\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type='post' # [4, 4, 5, 6, ..... 0, 0, 0] - zeros at the end \n",
    "oov_tok = \"<OOV>\" # out of vocabulary\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index # all 10000 words with tokens in a dictionary \n",
    "sequences = tokenizer.texts_to_sequences(training_sentences) # all sentences represented only with tokens\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type) # make all sentences the same size\n",
    "\n",
    "# the same for testing set\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)\n",
    "\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "? ? ? ? ? ? ? ? i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <OOV> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <OOV> without any real concern for anything else i cant recommend this film at all\n\nI have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(padded[1]))\n",
    "print()\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n>> original length 617\n>> label 0\n\n[11, 26, 75, 571, 6, 805, 2354, 313, 106, 19, 12, 7, 629, 686, 6, 4, 2219, 5, 181, 584, 64, 1454, 110, 2263, 3, 3951, 21, 2, 1, 3, 258, 41, 4677, 4, 174, 188, 21, 12, 4078, 11, 1578, 2354, 86, 2, 20, 14, 1907, 2, 112, 940, 14, 1811, 1340, 548, 3, 355, 181, 466, 6, 591, 19, 17, 55, 1817, 5, 49, 14, 4044, 96, 40, 136, 11, 972, 11, 201, 26, 1046, 171, 5, 2, 20, 19, 11, 294, 2, 2155, 5, 10, 3, 283, 41, 466, 6, 591, 5, 92, 203, 1, 207, 99, 145, 4382, 16, 230, 332, 11, 2486, 384, 12, 20, 31, 30]\n>> sequence lenght 112\n\n[   0    0    0    0    0    0    0    0   11   26   75  571    6  805\n 2354  313  106   19   12    7  629  686    6    4 2219    5  181  584\n   64 1454  110 2263    3 3951   21    2    1    3  258   41 4677    4\n  174  188   21   12 4078   11 1578 2354   86    2   20   14 1907    2\n  112  940   14 1811 1340  548    3  355  181  466    6  591   19   17\n   55 1817    5   49   14 4044   96   40  136   11  972   11  201   26\n 1046  171    5    2   20   19   11  294    2 2155    5   10    3  283\n   41  466    6  591    5   92  203    1  207   99  145 4382   16  230\n  332   11 2486  384   12   20   31   30]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "print(training_sentences[1]) \n",
    "print(\">> original length\", len(training_sentences[1]))\n",
    "print(\">> label\", training_labels[1])\n",
    "\n",
    "print()\n",
    "print(sequences[1])\n",
    "print(\">> sequence lenght\", len(sequences[1]))\n",
    "print()\n",
    "print(padded[1])\n",
    "padded[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'bintang'"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "# len(list(word_index)) # 90000 appr\n",
    "list(word_index)[57565] # even we defined vocab_size = 10000, tensorflow tokenizes all words, but in backed end it will work with 10000 words, \n",
    "# num_words=n parameter specifies the maximum number of words to be tokenized, and picks the most common ‘n’ words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 120, 16)           160000    \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 1920)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 6)                 11526     \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 7         \n=================================================================\nTotal params: 171,533\nTrainable params: 171,533\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(), # GlobalAveragePooling1D()\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 9.7313e-05 - accuracy: 1.0000 - val_loss: 0.8354 - val_accuracy: 0.8314\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.0164e-05 - accuracy: 1.0000 - val_loss: 0.8735 - val_accuracy: 0.8308\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.7304e-05 - accuracy: 1.0000 - val_loss: 0.9050 - val_accuracy: 0.8318\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.3330e-05 - accuracy: 1.0000 - val_loss: 0.9406 - val_accuracy: 0.8309\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 1.5115e-05 - accuracy: 1.0000 - val_loss: 0.9730 - val_accuracy: 0.8313\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 9.3207e-06 - accuracy: 1.0000 - val_loss: 1.0077 - val_accuracy: 0.8312\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 6.1326e-06 - accuracy: 1.0000 - val_loss: 1.0429 - val_accuracy: 0.8307\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 3.8306e-06 - accuracy: 1.0000 - val_loss: 1.0734 - val_accuracy: 0.8310\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.4845e-06 - accuracy: 1.0000 - val_loss: 1.1086 - val_accuracy: 0.8311\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 1.6163e-06 - accuracy: 1.0000 - val_loss: 1.1410 - val_accuracy: 0.8310\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29b991ee0>"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "# Training own modelg\n",
    "\n",
    "num_epochs = 10\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.embeddings.Embedding at 0x29bb18880>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x29bb18460>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x29bb18070>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x29bb20370>]"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "e = model.layers\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.08942658,  0.00486923, -0.05935808, -0.06226563, -0.04867279,\n",
       "        0.04237117,  0.04769849,  0.03356505, -0.03730453,  0.00785854,\n",
       "        0.03105144,  0.0776749 ,  0.05284716,  0.025134  , -0.03554538,\n",
       "       -0.04298926], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "weights[1] # each word has its own weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> word 1 <OOV>\n>> embeddings [-0.08942658  0.00486923 -0.05935808 -0.06226563 -0.04867279  0.04237117\n  0.04769849  0.03356505 -0.03730453  0.00785854  0.03105144  0.0776749\n  0.05284716  0.025134   -0.03554538 -0.04298926]\n>> word 2 the\n>> embeddings [-0.08670148  0.01641071 -0.02393427 -0.07146466  0.01603186  0.06126428\n  0.06148115  0.00766911  0.04187395  0.05556076  0.01930173  0.0744463\n  0.01907398  0.01339489  0.00941497 -0.0138381 ]\n>> word 3 and\n>> embeddings [ 0.01113727 -0.03538265 -0.05725451 -0.01636735 -0.00596739 -0.00635358\n  0.03053617  0.05559737  0.0871934   0.04494542  0.02274616  0.07229666\n  0.01994341  0.01223046 -0.05789011 -0.04256919]\n>> word 4 a\n>> embeddings [-0.05104827 -0.01813413 -0.04630557 -0.02343593 -0.03323779  0.06510878\n -0.00737528  0.02424134  0.0825871   0.00570629 -0.01472468  0.12047923\n  0.01702527 -0.04734353 -0.05681538 -0.06954415]\n"
     ]
    }
   ],
   "source": [
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  \n",
    "  if word_num < 5:\n",
    "    print(f\">> word {word_num}\", word)\n",
    "    print(\">> embeddings\", embeddings)\n",
    "\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n",
    "tf.config.list_physical_devices()"
   ]
  }
 ]
}