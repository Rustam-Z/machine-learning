{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Course 3 - Week 4 - Lesson 1 - Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOwsuGQQY9OL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRnDnCW-Z7qv",
        "outputId": "592936d3-5379-4901-ed54-299c965cf95b"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
        "\n",
        "corpus = data.lower().split(\"\\n\") # all sentences\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n",
            "263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1vZHpiN1AfF",
        "outputId": "fa6efcb1-c028-4cc5-f32f-eb2b167e8f1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(data.split(\"\\n\")[:5])\n",
        "print(len(data.split(\"\\n\"))) # number of all sentences\n",
        "print(type(data)) # str"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In the town of Athy one Jeremy Lanigan ', ' Battered away til he hadnt a pound. ', 'His father died and made him a man again ', ' Left him a farm and ten acres of ground. ', 'He gave a grand party for friends and relations ']\n",
            "64\n",
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOMYXjwGt1wQ",
        "outputId": "34159327-15c7-4565-b59e-7536faa6c1ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "corpus[:5] # it is the list containing all sent-s"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in the town of athy one jeremy lanigan ',\n",
              " ' battered away til he hadnt a pound. ',\n",
              " 'his father died and made him a man again ',\n",
              " ' left him a farm and ten acres of ground. ',\n",
              " 'he gave a grand party for friends and relations ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7f-CctFuHa3",
        "outputId": "4d026855-e1bb-4fe7-8b05-fb5785a6898d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.texts_to_sequences([\"in the town of athy one jeremy lanigan\"])[0] # if without [0] then it will be [[,,,,]]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 2, 66, 8, 67, 68, 69, 70]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtA8GpiEuoZT",
        "outputId": "f246f9e5-3e0e-4a1f-aaf6-aaefac07a46e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(line)\n",
        "print(corpus[-1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "and that put an end to lanigans ball.\n",
            "and that put an end to lanigans ball.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soPGVheskaQP"
      },
      "source": [
        "input_sequences = []\n",
        "\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "# input_sequences[:,:-1] everything besides last item\n",
        "# input_sequences[:,-1] last item\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYSaHxDzErO",
        "outputId": "061f958b-f414-479e-d489-56b7f4bf0249",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(input_sequences.shape)\n",
        "print(len(corpus))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(453, 11)\n",
            "64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44RmIiV5zVOU",
        "outputId": "ae773f51-8828-41ca-8bcf-3c7abfccd2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Look, input_sequences is the sliced version, each sentence created small subset of sentences depending on the number of words\n",
        "print(corpus[0])\n",
        "print(len(corpus[0].split())) # consist of 8 words"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in the town of athy one jeremy lanigan \n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5VH03sxxJ2M",
        "outputId": "0e976bbe-ed60-423e-f184-87a6d9a7c9d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_sequences[:8] # n words --> n-1 sets of small sent-s"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  2],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  4,  2, 66],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  4,  2, 66,  8],\n",
              "       [ 0,  0,  0,  0,  0,  0,  4,  2, 66,  8, 67],\n",
              "       [ 0,  0,  0,  0,  0,  4,  2, 66,  8, 67, 68],\n",
              "       [ 0,  0,  0,  0,  4,  2, 66,  8, 67, 68, 69],\n",
              "       [ 0,  0,  0,  4,  2, 66,  8, 67, 68, 69, 70],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 71, 40]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJtwVB2NbOAP",
        "outputId": "220a2af3-f72e-478d-95bd-b723f7d4e47c"
      },
      "source": [
        "print(tokenizer.word_index['in'])\n",
        "print(tokenizer.word_index['the'])\n",
        "print(tokenizer.word_index['town'])\n",
        "print(tokenizer.word_index['of'])\n",
        "print(tokenizer.word_index['athy'])\n",
        "print(tokenizer.word_index['one'])\n",
        "print(tokenizer.word_index['jeremy'])\n",
        "print(tokenizer.word_index['lanigan'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "2\n",
            "66\n",
            "8\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOkPViSqyMj0",
        "outputId": "0bed624c-969d-4f36-d0f8-f574c298bf21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(input_sequences[6])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  4  2 66  8 67 68 69 70]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Cv68JOakwv",
        "outputId": "8f883364-9af1-4ce4-f0e5-d95a76a5dbf7"
      },
      "source": [
        "print(xs[6])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  4  2 66  8 67 68 69]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY-jwvfgbEF8",
        "outputId": "97869e2c-9f2b-4859-af6b-b49b7cc263a5"
      },
      "source": [
        "print(ys[6]) # one-hot vector"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtzlUMYadhKt",
        "outputId": "adb7a8b1-c3ca-4769-f65b-9924e79e5be1"
      },
      "source": [
        "print(xs[5])\n",
        "print(ys[5])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  4  2 66  8 67 68]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4myRpB1c4Gg",
        "outputId": "35eff94e-6750-417f-e70b-786adbcf56c7"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xofU_YZB0cD-",
        "outputId": "fd416e55-101e-46dc-c1bd-c4cccc02322c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "total_words"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "263"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPX_1Wyi0ec1",
        "outputId": "cb3696df-6a85-452d-e059-9a6c8a3ebb07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "max_sequence_len"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9vH8Y59ajYL",
        "outputId": "0386d790-78f8-4278-ac35-3706953a263d"
      },
      "source": [
        "  model = Sequential()\n",
        "  model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "  model.add(Bidirectional(LSTM(20)))\n",
        "  model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(xs, ys, steps_per_epoch=453, epochs=500, verbose=1)\n",
        "\n",
        "  # xs+ys is one sentence, but as a label ys we have take the last word of a sent"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "453/453 [==============================] - 5s 5ms/step - loss: 5.5297 - accuracy: 0.0331\n",
            "Epoch 2/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 5.1250 - accuracy: 0.0442\n",
            "Epoch 3/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 4.8810 - accuracy: 0.0596\n",
            "Epoch 4/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 4.6931 - accuracy: 0.0773\n",
            "Epoch 5/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 4.4395 - accuracy: 0.0949\n",
            "Epoch 6/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 4.1586 - accuracy: 0.1435\n",
            "Epoch 7/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 3.8454 - accuracy: 0.1611\n",
            "Epoch 8/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 3.5325 - accuracy: 0.2053\n",
            "Epoch 9/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 3.2182 - accuracy: 0.2914\n",
            "Epoch 10/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 2.9843 - accuracy: 0.3377\n",
            "Epoch 11/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 2.7112 - accuracy: 0.3863\n",
            "Epoch 12/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 2.4764 - accuracy: 0.4592\n",
            "Epoch 13/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 2.2513 - accuracy: 0.5121\n",
            "Epoch 14/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 2.0561 - accuracy: 0.5695\n",
            "Epoch 15/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 1.8828 - accuracy: 0.5938\n",
            "Epoch 16/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 1.7238 - accuracy: 0.6446\n",
            "Epoch 17/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 1.5902 - accuracy: 0.6821\n",
            "Epoch 18/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 1.4505 - accuracy: 0.7329\n",
            "Epoch 19/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 1.3490 - accuracy: 0.7682\n",
            "Epoch 20/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 1.2476 - accuracy: 0.7837\n",
            "Epoch 21/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 1.1305 - accuracy: 0.8212\n",
            "Epoch 22/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 1.0499 - accuracy: 0.8124\n",
            "Epoch 23/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.9514 - accuracy: 0.8477\n",
            "Epoch 24/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.8815 - accuracy: 0.8720\n",
            "Epoch 25/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.8007 - accuracy: 0.8940\n",
            "Epoch 26/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.7479 - accuracy: 0.8940\n",
            "Epoch 27/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6858 - accuracy: 0.9051\n",
            "Epoch 28/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6415 - accuracy: 0.9161\n",
            "Epoch 29/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.5856 - accuracy: 0.9205\n",
            "Epoch 30/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.5218 - accuracy: 0.9360\n",
            "Epoch 31/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.5003 - accuracy: 0.9272\n",
            "Epoch 32/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.4457 - accuracy: 0.9382\n",
            "Epoch 33/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.4435 - accuracy: 0.9294\n",
            "Epoch 34/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.4129 - accuracy: 0.9360\n",
            "Epoch 35/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.3624 - accuracy: 0.9338\n",
            "Epoch 36/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.3466 - accuracy: 0.9338\n",
            "Epoch 37/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.3277 - accuracy: 0.9316\n",
            "Epoch 38/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.3019 - accuracy: 0.9360\n",
            "Epoch 39/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.2774 - accuracy: 0.9426\n",
            "Epoch 40/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.2639 - accuracy: 0.9404\n",
            "Epoch 41/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.2718 - accuracy: 0.9404\n",
            "Epoch 42/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.2357 - accuracy: 0.9360\n",
            "Epoch 43/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.2341 - accuracy: 0.9360\n",
            "Epoch 44/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.2139 - accuracy: 0.9404\n",
            "Epoch 45/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.2056 - accuracy: 0.9492\n",
            "Epoch 46/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.2479 - accuracy: 0.9205\n",
            "Epoch 47/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.2038 - accuracy: 0.9316\n",
            "Epoch 48/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1790 - accuracy: 0.9360\n",
            "Epoch 49/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1680 - accuracy: 0.9404\n",
            "Epoch 50/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1750 - accuracy: 0.9404\n",
            "Epoch 51/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1793 - accuracy: 0.9404\n",
            "Epoch 52/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1668 - accuracy: 0.9426\n",
            "Epoch 53/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1602 - accuracy: 0.9360\n",
            "Epoch 54/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1492 - accuracy: 0.9426\n",
            "Epoch 55/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1535 - accuracy: 0.9360\n",
            "Epoch 56/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1589 - accuracy: 0.9360\n",
            "Epoch 57/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1498 - accuracy: 0.9360\n",
            "Epoch 58/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1598 - accuracy: 0.9382\n",
            "Epoch 59/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1421 - accuracy: 0.9404\n",
            "Epoch 60/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.1334 - accuracy: 0.9338\n",
            "Epoch 61/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1283 - accuracy: 0.9426\n",
            "Epoch 62/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1276 - accuracy: 0.9338\n",
            "Epoch 63/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1249 - accuracy: 0.9382\n",
            "Epoch 64/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1231 - accuracy: 0.9360\n",
            "Epoch 65/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1218 - accuracy: 0.9404\n",
            "Epoch 66/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1747 - accuracy: 0.9316\n",
            "Epoch 67/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1577 - accuracy: 0.9272\n",
            "Epoch 68/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1260 - accuracy: 0.9448\n",
            "Epoch 69/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1197 - accuracy: 0.9360\n",
            "Epoch 70/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1163 - accuracy: 0.9338\n",
            "Epoch 71/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1159 - accuracy: 0.9404\n",
            "Epoch 72/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1160 - accuracy: 0.9316\n",
            "Epoch 73/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1165 - accuracy: 0.9382\n",
            "Epoch 74/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1260 - accuracy: 0.9426\n",
            "Epoch 75/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1350 - accuracy: 0.9338\n",
            "Epoch 76/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1569 - accuracy: 0.9294\n",
            "Epoch 77/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1207 - accuracy: 0.9382\n",
            "Epoch 78/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1131 - accuracy: 0.9360\n",
            "Epoch 79/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1119 - accuracy: 0.9382\n",
            "Epoch 80/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1129 - accuracy: 0.9426\n",
            "Epoch 81/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1120 - accuracy: 0.9360\n",
            "Epoch 82/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1112 - accuracy: 0.9382\n",
            "Epoch 83/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1113 - accuracy: 0.9404\n",
            "Epoch 84/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1116 - accuracy: 0.9448\n",
            "Epoch 85/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1132 - accuracy: 0.9404\n",
            "Epoch 86/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1177 - accuracy: 0.9382\n",
            "Epoch 87/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1271 - accuracy: 0.9360\n",
            "Epoch 88/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1148 - accuracy: 0.9382\n",
            "Epoch 89/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1108 - accuracy: 0.9448\n",
            "Epoch 90/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1174 - accuracy: 0.9426\n",
            "Epoch 91/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1114 - accuracy: 0.9426\n",
            "Epoch 92/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1089 - accuracy: 0.9426\n",
            "Epoch 93/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1103 - accuracy: 0.9360\n",
            "Epoch 94/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1131 - accuracy: 0.9382\n",
            "Epoch 95/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1097 - accuracy: 0.9426\n",
            "Epoch 96/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1093 - accuracy: 0.9360\n",
            "Epoch 97/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1110 - accuracy: 0.9404\n",
            "Epoch 98/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1086 - accuracy: 0.9404\n",
            "Epoch 99/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1117 - accuracy: 0.9360\n",
            "Epoch 100/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1148 - accuracy: 0.9360\n",
            "Epoch 101/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1117 - accuracy: 0.9382\n",
            "Epoch 102/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1075 - accuracy: 0.9448\n",
            "Epoch 103/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1074 - accuracy: 0.9382\n",
            "Epoch 104/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1053 - accuracy: 0.9426\n",
            "Epoch 105/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1077 - accuracy: 0.9338\n",
            "Epoch 106/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1062 - accuracy: 0.9382\n",
            "Epoch 107/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1076 - accuracy: 0.9382\n",
            "Epoch 108/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1436 - accuracy: 0.9338\n",
            "Epoch 109/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1607 - accuracy: 0.9227\n",
            "Epoch 110/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1327 - accuracy: 0.9294\n",
            "Epoch 111/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1119 - accuracy: 0.9448\n",
            "Epoch 112/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1067 - accuracy: 0.9404\n",
            "Epoch 113/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1052 - accuracy: 0.9426\n",
            "Epoch 114/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1061 - accuracy: 0.9448\n",
            "Epoch 115/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1050 - accuracy: 0.9360\n",
            "Epoch 116/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1067 - accuracy: 0.9338\n",
            "Epoch 117/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1066 - accuracy: 0.9316\n",
            "Epoch 118/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1043 - accuracy: 0.9448\n",
            "Epoch 119/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1059 - accuracy: 0.9338\n",
            "Epoch 120/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1050 - accuracy: 0.9426\n",
            "Epoch 121/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1044 - accuracy: 0.9404\n",
            "Epoch 122/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1043 - accuracy: 0.9426\n",
            "Epoch 123/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1041 - accuracy: 0.9404\n",
            "Epoch 124/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1044 - accuracy: 0.9338\n",
            "Epoch 125/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1069 - accuracy: 0.9360\n",
            "Epoch 126/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1167 - accuracy: 0.9382\n",
            "Epoch 127/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1099 - accuracy: 0.9338\n",
            "Epoch 128/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1295 - accuracy: 0.9338\n",
            "Epoch 129/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1181 - accuracy: 0.9360\n",
            "Epoch 130/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1059 - accuracy: 0.9338\n",
            "Epoch 131/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1057 - accuracy: 0.9426\n",
            "Epoch 132/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1063 - accuracy: 0.9360\n",
            "Epoch 133/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1039 - accuracy: 0.9448\n",
            "Epoch 134/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.1025 - accuracy: 0.9404\n",
            "Epoch 135/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1047 - accuracy: 0.9404\n",
            "Epoch 136/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1048 - accuracy: 0.9316\n",
            "Epoch 137/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1044 - accuracy: 0.9426\n",
            "Epoch 138/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1039 - accuracy: 0.9404\n",
            "Epoch 139/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1279 - accuracy: 0.9272\n",
            "Epoch 140/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1247 - accuracy: 0.9404\n",
            "Epoch 141/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1105 - accuracy: 0.9426\n",
            "Epoch 142/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1033 - accuracy: 0.9382\n",
            "Epoch 143/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1036 - accuracy: 0.9272\n",
            "Epoch 144/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1031 - accuracy: 0.9382\n",
            "Epoch 145/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1037 - accuracy: 0.9338\n",
            "Epoch 146/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1037 - accuracy: 0.9338\n",
            "Epoch 147/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1038 - accuracy: 0.9360\n",
            "Epoch 148/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1046 - accuracy: 0.9404\n",
            "Epoch 149/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1030 - accuracy: 0.9404\n",
            "Epoch 150/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1030 - accuracy: 0.9404\n",
            "Epoch 151/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1030 - accuracy: 0.9404\n",
            "Epoch 152/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1031 - accuracy: 0.9492\n",
            "Epoch 153/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1084 - accuracy: 0.9448\n",
            "Epoch 154/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1047 - accuracy: 0.9338\n",
            "Epoch 155/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1164 - accuracy: 0.9448\n",
            "Epoch 156/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1039 - accuracy: 0.9382\n",
            "Epoch 157/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1022 - accuracy: 0.9382\n",
            "Epoch 158/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1010 - accuracy: 0.9360\n",
            "Epoch 159/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1019 - accuracy: 0.9360\n",
            "Epoch 160/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.1018 - accuracy: 0.9404\n",
            "Epoch 161/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1031 - accuracy: 0.9426\n",
            "Epoch 162/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.1015 - accuracy: 0.9426\n",
            "Epoch 163/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1028 - accuracy: 0.9382\n",
            "Epoch 164/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1008 - accuracy: 0.9404\n",
            "Epoch 165/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1232 - accuracy: 0.9294\n",
            "Epoch 166/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1278 - accuracy: 0.9294\n",
            "Epoch 167/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1332 - accuracy: 0.9316\n",
            "Epoch 168/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1041 - accuracy: 0.9382\n",
            "Epoch 169/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1012 - accuracy: 0.9338\n",
            "Epoch 170/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1021 - accuracy: 0.9360\n",
            "Epoch 171/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.1015 - accuracy: 0.9404\n",
            "Epoch 172/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1007 - accuracy: 0.9360\n",
            "Epoch 173/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1025 - accuracy: 0.9426\n",
            "Epoch 174/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.1011 - accuracy: 0.9426\n",
            "Epoch 175/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1018 - accuracy: 0.9426\n",
            "Epoch 176/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1008 - accuracy: 0.9360\n",
            "Epoch 177/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1004 - accuracy: 0.9338\n",
            "Epoch 178/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1007 - accuracy: 0.9382\n",
            "Epoch 179/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1015 - accuracy: 0.9338\n",
            "Epoch 180/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1044 - accuracy: 0.9404\n",
            "Epoch 181/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1035 - accuracy: 0.9382\n",
            "Epoch 182/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1028 - accuracy: 0.9382\n",
            "Epoch 183/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1169 - accuracy: 0.9382\n",
            "Epoch 184/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1082 - accuracy: 0.9360\n",
            "Epoch 185/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1008 - accuracy: 0.9448\n",
            "Epoch 186/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1003 - accuracy: 0.9382\n",
            "Epoch 187/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1008 - accuracy: 0.9360\n",
            "Epoch 188/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1003 - accuracy: 0.9426\n",
            "Epoch 189/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1002 - accuracy: 0.9338\n",
            "Epoch 190/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1006 - accuracy: 0.9382\n",
            "Epoch 191/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1009 - accuracy: 0.9404\n",
            "Epoch 192/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1011 - accuracy: 0.9426\n",
            "Epoch 193/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1007 - accuracy: 0.9426\n",
            "Epoch 194/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0999 - accuracy: 0.9426\n",
            "Epoch 195/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.1008 - accuracy: 0.9360\n",
            "Epoch 196/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1007 - accuracy: 0.9382\n",
            "Epoch 197/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1013 - accuracy: 0.9448\n",
            "Epoch 198/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1010 - accuracy: 0.9316\n",
            "Epoch 199/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1009 - accuracy: 0.9382\n",
            "Epoch 200/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1022 - accuracy: 0.9404\n",
            "Epoch 201/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1300 - accuracy: 0.9294\n",
            "Epoch 202/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1010 - accuracy: 0.9382\n",
            "Epoch 203/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1006 - accuracy: 0.9404\n",
            "Epoch 204/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1005 - accuracy: 0.9404\n",
            "Epoch 205/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1004 - accuracy: 0.9360\n",
            "Epoch 206/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1003 - accuracy: 0.9382\n",
            "Epoch 207/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1003 - accuracy: 0.9448\n",
            "Epoch 208/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0996 - accuracy: 0.9426\n",
            "Epoch 209/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1006 - accuracy: 0.9360\n",
            "Epoch 210/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1003 - accuracy: 0.9338\n",
            "Epoch 211/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1008 - accuracy: 0.9426\n",
            "Epoch 212/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1005 - accuracy: 0.9448\n",
            "Epoch 213/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0979 - accuracy: 0.9404\n",
            "Epoch 214/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1012 - accuracy: 0.9360\n",
            "Epoch 215/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1175 - accuracy: 0.9360\n",
            "Epoch 216/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1009 - accuracy: 0.9382\n",
            "Epoch 217/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1008 - accuracy: 0.9382\n",
            "Epoch 218/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1001 - accuracy: 0.9404\n",
            "Epoch 219/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1003 - accuracy: 0.9338\n",
            "Epoch 220/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0996 - accuracy: 0.9404\n",
            "Epoch 221/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0990 - accuracy: 0.9360\n",
            "Epoch 222/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0999 - accuracy: 0.9382\n",
            "Epoch 223/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0989 - accuracy: 0.9448\n",
            "Epoch 224/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1003 - accuracy: 0.9316\n",
            "Epoch 225/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0997 - accuracy: 0.9426\n",
            "Epoch 226/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1015 - accuracy: 0.9360\n",
            "Epoch 227/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0997 - accuracy: 0.9360\n",
            "Epoch 228/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1005 - accuracy: 0.9360\n",
            "Epoch 229/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0999 - accuracy: 0.9382\n",
            "Epoch 230/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0991 - accuracy: 0.9382\n",
            "Epoch 231/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1006 - accuracy: 0.9382\n",
            "Epoch 232/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1139 - accuracy: 0.9360\n",
            "Epoch 233/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1030 - accuracy: 0.9426\n",
            "Epoch 234/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1081 - accuracy: 0.9294\n",
            "Epoch 235/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0993 - accuracy: 0.9426\n",
            "Epoch 236/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0996 - accuracy: 0.9404\n",
            "Epoch 237/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0989 - accuracy: 0.9448\n",
            "Epoch 238/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0994 - accuracy: 0.9360\n",
            "Epoch 239/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0987 - accuracy: 0.9382\n",
            "Epoch 240/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0995 - accuracy: 0.9360\n",
            "Epoch 241/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0997 - accuracy: 0.9360\n",
            "Epoch 242/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0998 - accuracy: 0.9470\n",
            "Epoch 243/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0994 - accuracy: 0.9360\n",
            "Epoch 244/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0998 - accuracy: 0.9338\n",
            "Epoch 245/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0994 - accuracy: 0.9382\n",
            "Epoch 246/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0995 - accuracy: 0.9448\n",
            "Epoch 247/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1001 - accuracy: 0.9338\n",
            "Epoch 248/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1016 - accuracy: 0.9360\n",
            "Epoch 249/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1091 - accuracy: 0.9404\n",
            "Epoch 250/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1054 - accuracy: 0.9448\n",
            "Epoch 251/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1007 - accuracy: 0.9426\n",
            "Epoch 252/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1013 - accuracy: 0.9294\n",
            "Epoch 253/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1004 - accuracy: 0.9448\n",
            "Epoch 254/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1006 - accuracy: 0.9360\n",
            "Epoch 255/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0999 - accuracy: 0.9316\n",
            "Epoch 256/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1001 - accuracy: 0.9382\n",
            "Epoch 257/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1000 - accuracy: 0.9426\n",
            "Epoch 258/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0994 - accuracy: 0.9316\n",
            "Epoch 259/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1002 - accuracy: 0.9360\n",
            "Epoch 260/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0992 - accuracy: 0.9360\n",
            "Epoch 261/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0994 - accuracy: 0.9404\n",
            "Epoch 262/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0994 - accuracy: 0.9382\n",
            "Epoch 263/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0989 - accuracy: 0.9382\n",
            "Epoch 264/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1001 - accuracy: 0.9338\n",
            "Epoch 265/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0988 - accuracy: 0.9360\n",
            "Epoch 266/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0990 - accuracy: 0.9404\n",
            "Epoch 267/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0994 - accuracy: 0.9448\n",
            "Epoch 268/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0988 - accuracy: 0.9448\n",
            "Epoch 269/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0981 - accuracy: 0.9360\n",
            "Epoch 270/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1020 - accuracy: 0.9382\n",
            "Epoch 271/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0991 - accuracy: 0.9426\n",
            "Epoch 272/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0985 - accuracy: 0.9404\n",
            "Epoch 273/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0983 - accuracy: 0.9426\n",
            "Epoch 274/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0987 - accuracy: 0.9316\n",
            "Epoch 275/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0976 - accuracy: 0.9404\n",
            "Epoch 276/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0990 - accuracy: 0.9382\n",
            "Epoch 277/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0984 - accuracy: 0.9404\n",
            "Epoch 278/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0989 - accuracy: 0.9404\n",
            "Epoch 279/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1089 - accuracy: 0.9382\n",
            "Epoch 280/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0998 - accuracy: 0.9404\n",
            "Epoch 281/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0978 - accuracy: 0.9360\n",
            "Epoch 282/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0988 - accuracy: 0.9448\n",
            "Epoch 283/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0984 - accuracy: 0.9426\n",
            "Epoch 284/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0985 - accuracy: 0.9316\n",
            "Epoch 285/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.0975 - accuracy: 0.9448\n",
            "Epoch 286/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0984 - accuracy: 0.9360\n",
            "Epoch 287/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0970 - accuracy: 0.9382\n",
            "Epoch 288/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0976 - accuracy: 0.9404\n",
            "Epoch 289/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0974 - accuracy: 0.9426\n",
            "Epoch 290/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0977 - accuracy: 0.9382\n",
            "Epoch 291/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0981 - accuracy: 0.9426\n",
            "Epoch 292/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0986 - accuracy: 0.9404\n",
            "Epoch 293/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0980 - accuracy: 0.9382\n",
            "Epoch 294/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0980 - accuracy: 0.9404\n",
            "Epoch 295/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0981 - accuracy: 0.9360\n",
            "Epoch 296/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1089 - accuracy: 0.9338\n",
            "Epoch 297/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1115 - accuracy: 0.9316\n",
            "Epoch 298/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1029 - accuracy: 0.9338\n",
            "Epoch 299/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0983 - accuracy: 0.9470\n",
            "Epoch 300/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0979 - accuracy: 0.9338\n",
            "Epoch 301/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0976 - accuracy: 0.9426\n",
            "Epoch 302/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0973 - accuracy: 0.9448\n",
            "Epoch 303/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0974 - accuracy: 0.9382\n",
            "Epoch 304/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0975 - accuracy: 0.9404\n",
            "Epoch 305/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0972 - accuracy: 0.9426\n",
            "Epoch 306/500\n",
            "453/453 [==============================] - 2s 6ms/step - loss: 0.0982 - accuracy: 0.9360\n",
            "Epoch 307/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0978 - accuracy: 0.9382\n",
            "Epoch 308/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0971 - accuracy: 0.9470\n",
            "Epoch 309/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0980 - accuracy: 0.9316\n",
            "Epoch 310/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0981 - accuracy: 0.9316\n",
            "Epoch 311/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0982 - accuracy: 0.9426\n",
            "Epoch 312/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0972 - accuracy: 0.9404\n",
            "Epoch 313/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0988 - accuracy: 0.9338\n",
            "Epoch 314/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0981 - accuracy: 0.9404\n",
            "Epoch 315/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0976 - accuracy: 0.9404\n",
            "Epoch 316/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0960 - accuracy: 0.9426\n",
            "Epoch 317/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0974 - accuracy: 0.9382\n",
            "Epoch 318/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0972 - accuracy: 0.9404\n",
            "Epoch 319/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1048 - accuracy: 0.9338\n",
            "Epoch 320/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0973 - accuracy: 0.9338\n",
            "Epoch 321/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0971 - accuracy: 0.9492\n",
            "Epoch 322/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0976 - accuracy: 0.9382\n",
            "Epoch 323/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0977 - accuracy: 0.9360\n",
            "Epoch 324/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0970 - accuracy: 0.9404\n",
            "Epoch 325/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0974 - accuracy: 0.9360\n",
            "Epoch 326/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0974 - accuracy: 0.9382\n",
            "Epoch 327/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0974 - accuracy: 0.9404\n",
            "Epoch 328/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0974 - accuracy: 0.9426\n",
            "Epoch 329/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0976 - accuracy: 0.9448\n",
            "Epoch 330/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0979 - accuracy: 0.9404\n",
            "Epoch 331/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0968 - accuracy: 0.9426\n",
            "Epoch 332/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0975 - accuracy: 0.9404\n",
            "Epoch 333/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0963 - accuracy: 0.9404\n",
            "Epoch 334/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0967 - accuracy: 0.9448\n",
            "Epoch 335/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0980 - accuracy: 0.9382\n",
            "Epoch 336/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1014 - accuracy: 0.9426\n",
            "Epoch 337/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1142 - accuracy: 0.9294\n",
            "Epoch 338/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0991 - accuracy: 0.9382\n",
            "Epoch 339/500\n",
            "453/453 [==============================] - 3s 7ms/step - loss: 0.1026 - accuracy: 0.9360\n",
            "Epoch 340/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0973 - accuracy: 0.9448\n",
            "Epoch 341/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0979 - accuracy: 0.9316\n",
            "Epoch 342/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0967 - accuracy: 0.9338\n",
            "Epoch 343/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0969 - accuracy: 0.9338\n",
            "Epoch 344/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9382\n",
            "Epoch 345/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0968 - accuracy: 0.9448\n",
            "Epoch 346/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0970 - accuracy: 0.9382\n",
            "Epoch 347/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0976 - accuracy: 0.9426\n",
            "Epoch 348/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0975 - accuracy: 0.9382\n",
            "Epoch 349/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0970 - accuracy: 0.9360\n",
            "Epoch 350/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1010 - accuracy: 0.9360\n",
            "Epoch 351/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0983 - accuracy: 0.9382\n",
            "Epoch 352/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0972 - accuracy: 0.9448\n",
            "Epoch 353/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0966 - accuracy: 0.9360\n",
            "Epoch 354/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0963 - accuracy: 0.9360\n",
            "Epoch 355/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0965 - accuracy: 0.9448\n",
            "Epoch 356/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0965 - accuracy: 0.9360\n",
            "Epoch 357/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0972 - accuracy: 0.9360\n",
            "Epoch 358/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0968 - accuracy: 0.9426\n",
            "Epoch 359/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0967 - accuracy: 0.9448\n",
            "Epoch 360/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9470\n",
            "Epoch 361/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0957 - accuracy: 0.9404\n",
            "Epoch 362/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0971 - accuracy: 0.9404\n",
            "Epoch 363/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0972 - accuracy: 0.9382\n",
            "Epoch 364/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0981 - accuracy: 0.9338\n",
            "Epoch 365/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1041 - accuracy: 0.9360\n",
            "Epoch 366/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0966 - accuracy: 0.9382\n",
            "Epoch 367/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0969 - accuracy: 0.9426\n",
            "Epoch 368/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0971 - accuracy: 0.9448\n",
            "Epoch 369/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0966 - accuracy: 0.9426\n",
            "Epoch 370/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0963 - accuracy: 0.9404\n",
            "Epoch 371/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9426\n",
            "Epoch 372/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0971 - accuracy: 0.9382\n",
            "Epoch 373/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0968 - accuracy: 0.9382\n",
            "Epoch 374/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0966 - accuracy: 0.9404\n",
            "Epoch 375/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0972 - accuracy: 0.9426\n",
            "Epoch 376/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0970 - accuracy: 0.9404\n",
            "Epoch 377/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0971 - accuracy: 0.9338\n",
            "Epoch 378/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0960 - accuracy: 0.9404\n",
            "Epoch 379/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9404\n",
            "Epoch 380/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9316\n",
            "Epoch 381/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9382\n",
            "Epoch 382/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1001 - accuracy: 0.9470\n",
            "Epoch 383/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0977 - accuracy: 0.9426\n",
            "Epoch 384/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0969 - accuracy: 0.9448\n",
            "Epoch 385/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9338\n",
            "Epoch 386/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0958 - accuracy: 0.9338\n",
            "Epoch 387/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9404\n",
            "Epoch 388/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9448\n",
            "Epoch 389/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9316\n",
            "Epoch 390/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0965 - accuracy: 0.9360\n",
            "Epoch 391/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9382\n",
            "Epoch 392/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0957 - accuracy: 0.9338\n",
            "Epoch 393/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0968 - accuracy: 0.9316\n",
            "Epoch 394/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9382\n",
            "Epoch 395/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0967 - accuracy: 0.9492\n",
            "Epoch 396/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0968 - accuracy: 0.9404\n",
            "Epoch 397/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9382\n",
            "Epoch 398/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0966 - accuracy: 0.9404\n",
            "Epoch 399/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9426\n",
            "Epoch 400/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9404\n",
            "Epoch 401/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9382\n",
            "Epoch 402/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0958 - accuracy: 0.9338\n",
            "Epoch 403/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0968 - accuracy: 0.9470\n",
            "Epoch 404/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0973 - accuracy: 0.9404\n",
            "Epoch 405/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0969 - accuracy: 0.9360\n",
            "Epoch 406/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9360\n",
            "Epoch 407/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0963 - accuracy: 0.9404\n",
            "Epoch 408/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.0956 - accuracy: 0.9382\n",
            "Epoch 409/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9360\n",
            "Epoch 410/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0957 - accuracy: 0.9404\n",
            "Epoch 411/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9338\n",
            "Epoch 412/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9382\n",
            "Epoch 413/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0968 - accuracy: 0.9448\n",
            "Epoch 414/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0965 - accuracy: 0.9426\n",
            "Epoch 415/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1123 - accuracy: 0.9272\n",
            "Epoch 416/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9338\n",
            "Epoch 417/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9360\n",
            "Epoch 418/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9448\n",
            "Epoch 419/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9448\n",
            "Epoch 420/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9360\n",
            "Epoch 421/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9382\n",
            "Epoch 422/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9382\n",
            "Epoch 423/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9360\n",
            "Epoch 424/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9426\n",
            "Epoch 425/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9470\n",
            "Epoch 426/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9448\n",
            "Epoch 427/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0965 - accuracy: 0.9338\n",
            "Epoch 428/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0963 - accuracy: 0.9448\n",
            "Epoch 429/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0958 - accuracy: 0.9426\n",
            "Epoch 430/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9382\n",
            "Epoch 431/500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.1069 - accuracy: 0.9426\n",
            "Epoch 432/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1097 - accuracy: 0.9360\n",
            "Epoch 433/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0963 - accuracy: 0.9360\n",
            "Epoch 434/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9426\n",
            "Epoch 435/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9404\n",
            "Epoch 436/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9382\n",
            "Epoch 437/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0957 - accuracy: 0.9470\n",
            "Epoch 438/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9360\n",
            "Epoch 439/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9426\n",
            "Epoch 440/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0953 - accuracy: 0.9382\n",
            "Epoch 441/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9426\n",
            "Epoch 442/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9294\n",
            "Epoch 443/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9382\n",
            "Epoch 444/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9338\n",
            "Epoch 445/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9404\n",
            "Epoch 446/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0954 - accuracy: 0.9382\n",
            "Epoch 447/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0958 - accuracy: 0.9382\n",
            "Epoch 448/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0966 - accuracy: 0.9404\n",
            "Epoch 449/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9316\n",
            "Epoch 450/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0960 - accuracy: 0.9338\n",
            "Epoch 451/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1147 - accuracy: 0.9382\n",
            "Epoch 452/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1022 - accuracy: 0.9382\n",
            "Epoch 453/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9382\n",
            "Epoch 454/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0960 - accuracy: 0.9448\n",
            "Epoch 455/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0954 - accuracy: 0.9360\n",
            "Epoch 456/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0957 - accuracy: 0.9382\n",
            "Epoch 457/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9404\n",
            "Epoch 458/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0963 - accuracy: 0.9338\n",
            "Epoch 459/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0960 - accuracy: 0.9294\n",
            "Epoch 460/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0954 - accuracy: 0.9448\n",
            "Epoch 461/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0946 - accuracy: 0.9426\n",
            "Epoch 462/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9470\n",
            "Epoch 463/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9426\n",
            "Epoch 464/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9382\n",
            "Epoch 465/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9360\n",
            "Epoch 466/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0966 - accuracy: 0.9382\n",
            "Epoch 467/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0947 - accuracy: 0.9360\n",
            "Epoch 468/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0962 - accuracy: 0.9382\n",
            "Epoch 469/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0954 - accuracy: 0.9426\n",
            "Epoch 470/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0950 - accuracy: 0.9470\n",
            "Epoch 471/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9448\n",
            "Epoch 472/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9316\n",
            "Epoch 473/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0960 - accuracy: 0.9360\n",
            "Epoch 474/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0964 - accuracy: 0.9448\n",
            "Epoch 475/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9338\n",
            "Epoch 476/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9338\n",
            "Epoch 477/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9316\n",
            "Epoch 478/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0967 - accuracy: 0.9404\n",
            "Epoch 479/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0958 - accuracy: 0.9360\n",
            "Epoch 480/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0974 - accuracy: 0.9426\n",
            "Epoch 481/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0949 - accuracy: 0.9338\n",
            "Epoch 482/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.1047 - accuracy: 0.9404\n",
            "Epoch 483/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9360\n",
            "Epoch 484/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0953 - accuracy: 0.9426\n",
            "Epoch 485/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9360\n",
            "Epoch 486/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0957 - accuracy: 0.9382\n",
            "Epoch 487/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0957 - accuracy: 0.9360\n",
            "Epoch 488/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0958 - accuracy: 0.9382\n",
            "Epoch 489/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9448\n",
            "Epoch 490/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9404\n",
            "Epoch 491/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9272\n",
            "Epoch 492/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0948 - accuracy: 0.9404\n",
            "Epoch 493/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9426\n",
            "Epoch 494/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0956 - accuracy: 0.9404\n",
            "Epoch 495/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9404\n",
            "Epoch 496/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0950 - accuracy: 0.9382\n",
            "Epoch 497/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9426\n",
            "Epoch 498/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0957 - accuracy: 0.9382\n",
            "Epoch 499/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9426\n",
            "Epoch 500/500\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.0948 - accuracy: 0.9360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YXGelKThoTT"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "poeprYK8h-c7",
        "outputId": "7c75b81e-ff31-4bb5-f3ad-6938f8ffdaf0"
      },
      "source": [
        "plot_graphs(history, 'accuracy')\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vc6PHgxa6Hm",
        "outputId": "3a5cb74c-a9fb-4410-b6cf-b0a1502cb6bf"
      },
      "source": [
        "seed_text = \"Laurence went to dublin\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Laurence went to dublin girls and i soon both friends and relations relations relations their dublin their wine for all cask the round a ladies ladies lads mavrone hearty cask miss kerrigan fainted for all entangled entangled entangled dancing boys boys rows and lads twas was a went ask ladies ladies her cheeks as red ask he as merry as water water plenty as water water water water singing lanigans ball singing lanigans ball red stepped all mad at me a a jig a jig jig relations a jig taras grand hall hall taras soon both friends and relations relations relations and all the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pbyXd8_0zOZ"
      },
      "source": [
        "  model = Sequential()\n",
        "  model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "  model.add(LSTM(20))\n",
        "  model.add(Dense(total_words, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt"
      ]
    }
  ]
}